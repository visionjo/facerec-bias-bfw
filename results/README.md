# Results generated in this paper

Files containing the results generated by the source code in the `code` folder.

* `datatable.csv`:
        Generated by

### Experiments
#### 1. Signal Detection Models (SDM)
<img src=../manuscript/figures/violinplots.png alt="violinplots.png" width="500"/>

#### 2. NN (i.e., Rank 1) Error Analysis
<img src=../manuscript/figures/confusion.png alt="confusion.png" width="500"/>

#### 3. Detection Error Trade-Off (DET) Curves
**DET curves.** Top-left: per gender.Top-Right: per ethnic groups.Bottom:per subgroup. Lower is better. On the gender-based DET curve, two dashed lined connects points at a constan threshold, which shows a rough factor of 3 difference inFPR at the same threshold. Similar differences in FPR exist in the other plots at a constant threshold &theta;<sub>const</sub>.

<img src=../manuscript/figures/detcurve-improved.png alt="detcurve" width="500"/>

#### 4. TAR at FAR
<img src=../manuscript/figures/tar_at_far.png alt="tar_at_far.png" width="400"/>

#### 5. Human Evaluation
We evaluated human on face pairs focusing on two racial groups: Chinese and Caucasians. To focus on the experiment, we honed-in on two groups, white Americans (W) and Chinese from China (C). The purpose was to the minimize variability by only analyzing the subsets of the broader groups of whites and Asians.
Samples were collected by recruiting subjects from multiple sources (e.g., social media, email lists, and family/friends)– a total of 120 par- ticipants were sampled at random from all the submissions that were (1) complete and (2) from a W or C participant. Specifically, there were 60 W and 60 C, both with Male (M) and Female (F) split evenly. A total of 50 face pairs of non-famous
“look-alikes” were collected from the internet, with 20 (WA) and 20 (C) pairs (male and female split evenly). The other 10 pairs are of others (e.g., Hispanic/ Latino, Japanese, African). Survey was created, distributed, and recorded via PaperForm.

<img src=../manuscript/figures/human-eval-table.png alt="humaneval-fig" width="400"/>

Quantitative and qualitative results are in Table IV and Fig. 5, respectfully. One might expect that the most exposure to others would be within the same subgroup, and, therefore, would be best at labeling their own. Secondarily, they would be best at labeling images of the same ethnicity, but opposite gender. Our findings concur. Each subgroup is best at labeling their type, and then second best at labeling the same ethnicity but opposite sex. Interestingly, each group of images is best tagged by the corresponding subgroup, with the second-to-best having the same ethnicity and opposite gender. On average, subgroups are comparable at labeling images.

<img src=../manuscript/figures/human-eval-fig.png alt="humaneval-fig" width="400"/>

### Supplemental
Variations in optimal threshold are shown shown to generalize to other models (Fig. 6). Like in Fig. 2, the Detection Error Trade-off (DET) curves for three CNN-based models, each trained on VGG2 with softmax but with different backbones.

<img src=../manuscript/figures/multi-models-det.png alt="multi-models-det" width="800"/>
